| Field                                                         | Display Name                               | Tooltip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|---------------------------------------------------------------|--------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Stream Chat Response                                          | Stream Response                            | Show the answer as it’s being generated (streaming) instead of waiting for the full response to finish.<br>This makes replies appear in real-time. Usually leave this On for faster feedback; turn it Off only if a model doesn’t support streaming or you prefer to receive the answer all at once.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Function Calling                                              | Enable Function Calling                    | Allow the model to output a JSON-formatted function call to use an external tool or function instead of a normal text answer. Useful only if you have defined custom functions for the model (e.g. a calculator or web search). Keep this off unless you want the AI to potentially call a function you’ve set up; enabling it without defined functions has no benefit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Seed                                                          | Random Seed                                | Sets a specific starting seed for the model’s random number generator. Using the same seed with the same prompt ensures the exact same response each time (good for testing and comparisons). By default a random seed is used, so you get variety. Change this to a fixed number if you need repeatable results; otherwise leave it random for diverse outputs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Stop Sequence                                                 | Stop Sequence                              | Defines a token or string where the model should stop generating further text. When the model encounters this sequence in its output, it will end the response. Use this to cut off answers at a desired point (for example, after a closing tag or specific word). If left blank, the model will stop on its own (end-of-text or max tokens) – adjust this only if you need the output to terminate at a custom spot.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Temperature                                                   | Temperature (Creativity)                   | Controls randomness in output. Lower values (e.g. 0.2) make the model more focused and deterministic (good for factual or strict tasks), while higher values (e.g. 0.8) produce more varied and creative responses. The default (around 0.7) is a balanced setting. Increase temperature for imaginative or diverse answers; decrease it for predictable, reliable answers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Reasoning Effort                                              | Reasoning Effort                           | Sets how much “thinking out loud” the model does internally. Low effort means minimal reasoning steps (faster responses that are more to-the-point), while High effort means the model spends more time and tokens reasoning (slower, more detailed answers). Use a higher setting for complex problems or when you want detailed, step-by-step explanations; use low/medium for quicker, concise replies. If unsure, leave at default (Medium).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Logit Bias                                                    | Token Bias (Logit Bias)                    | Advanced setting to nudge the model toward or away from specific words/tokens. You provide a list of token IDs or words with bias values – positive bias makes a token more likely, negative makes it less likely (extreme values can force or ban a token). Use this to prevent unwanted words or promote a certain style. Typically left empty (no bias) for normal use; only adjust if you have particular tokens you need to control in the output.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Mirostat                                                      | Mirostat Sampling                          | Enables Mirostat, an adaptive sampling algorithm that dynamically adjusts randomness to maintain a target level of surprise (perplexity) in the text. This helps balance coherence and creativity during generation, especially for long outputs. By default it’s off. You might turn this on if you find that with normal sampling the model’s output becomes incoherent in long responses – Mirostat will attempt to keep it on track. (Mode 1 and 2 correspond to two versions of the algorithm.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Mirostat Eta                                                  | Mirostat Learning Rate (Eta)               | How quickly Mirostat adapts its sampling based on the generated text. A lower eta means slower, gradual adjustments (the model changes its randomness more conservatively), while a higher eta makes it respond faster to differences, adjusting randomness more aggressively. Only has an effect if Mirostat is enabled. If using Mirostat, you might lower this for more stable adjustments or increase it if the model isn’t meeting the target perplexity fast enough. The default (0.1) is usually sensible.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Mirostat Tau                                                  | Mirostat Target (Tau)                      | The target “surprise” level (perplexity) for Mirostat sampling. Lower tau values aim for more focused and coherent text (model keeps things predictable), while higher tau allows more diverse and unexpected word choices. Only relevant when Mirostat is on. Adjust this if you’re using Mirostat and want to tune how creative vs. coherent the output should be. The default (around 5.0) is a moderate balance; lower it for safer, more on-topic responses or raise it for more variety.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Top K                                                         | Top-K Sampling                             | Limits the model to considering only the top K most likely tokens at each step. A higher K (e.g. 100) means the model has more options (potentially more diverse or surprising outputs), while a lower K (e.g. 10) makes it pick from a small set of likely tokens (more conservative and focused output). Default is often around 40. You might lower Top-K if the model is rambling or making odd choices, to restrict it to the best candidates. Increase it if you want more randomness or creativity (in combination with Temperature).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Top P                                                         | Top-P Sampling (Nucleus)                   | Instead of a fixed number of tokens, this limits the model to a dynamic set of tokens that comprise a certain cumulative probability P of the distribution. For example, at P = 0.9, the model considers the smallest group of tokens whose probabilities add up to 90%. Higher P (closer to 1.0) lets more tokens through (more variety), lower P (e.g. 0.5) makes output more focused on the top-ranked tokens. Typically 0.9 is default. Adjust this along with Temperature: decrease P for stricter, more relevant answers; increase it for more creative freedom.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Min P                                                         | Min Probability (Min P)                    | Filters out extremely unlikely tokens based on a probability threshold. This is essentially the inverse of Top-P: any token with probability lower than (min_p × probability of the top token) will be excluded from choices. For example, at min_p = 0.05, if the best token has a 0.9 probability, tokens with probability < 0.045 are not considered. This can help eliminate the “long tail” of bizarre options. Usually left at 0 (disabled) by default; you might set a small value (e.g. 0.01–0.1) if you want to avoid the model ever choosing very low-probability words (for more reliable output). Use with care, as too high a min_p might make text repetitive or simplistic.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Frequency Penalty                                            | Frequency Penalty                          | Discourages the model from overusing the same words repeatedly in its output. This penalty increases with each occurrence of a token – the more frequently a word has appeared, the more the model is dissuaded from using it again. In effect, it says “you’ve used that word a lot already, try something else now”. A higher value means fewer repeats (the model will avoid repeating any word too often). Keep it at 0 for no effect (default). If you notice the AI repeating phrases or sticking to the same wording, you can raise this slightly (e.g. 0.5 or 1.0) to encourage more varied vocabulary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Presence Penalty                                             | Presence Penalty                           | Prevents the model from repeating any word that has already appeared at least once. Essentially, once a token has been used, its probability is immediately lowered for the rest of the response – “you’ve said that word before, don’t say it again.” This is a stricter anti-repetition measure than Frequency Penalty. Use it if you want to strongly avoid any reuse of words (for maximum diversity). Typically left at 0 (off) because it can make the model paraphrase heavily; increase it carefully (e.g. 0.2–0.5) if the output is overly repetitive and you want to eliminate duplicates completely.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Repeat Last N                                                 | Repetition Memory (Last N)                 | How far back the model checks for repeated text when applying repetition penalties. This sets the number of recent tokens to remember for the purpose of avoiding repetition. For example, if set to 64 (the default), the model will look at roughly the last 64 tokens it generated and try not to repeat any sequence within that window. A larger value (or –1 for full context) means it considers a longer history, so it will avoid echoing phrases even from earlier in the conversation (useful for very long outputs, but can make the model forget or alter earlier phrasing). A smaller value or 0 limits the repetition check to a shorter span (or disables it), which can sometimes be useful for creative writing where repeating certain words for style is okay. In general, leave this at the default unless you need to fine-tune how the model handles repetition over long texts. |
| Tfs Z                                                         | Tail-Free Sampling (TFS‑Z)                 | Tail‑Free Sampling parameter to reduce the impact of low‑probability “tail” tokens in generation. A value of 1.0 effectively disables this (no tail filtering), while higher values (e.g. 2.0) progressively filter out more unlikely tokens. In practice, increasing TFS‑Z makes the output more focused and coherent by eliminating fringe possibilities, at the risk of losing some creativity. This is an advanced setting – the default (often 1.0, meaning off) is fine for most cases. You might experiment with a slight increase if the model’s outputs contain random or nonsensical words; otherwise, it’s safe to leave this unchanged.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Tokens To Keep On Context Refresh (num_keep)                 | Keep Tokens on Refresh                      | When the conversation context is too long and gets trimmed, this setting keeps a certain number of initial tokens so they are not forgotten. In other words, if the chat history hits the maximum context length and the model’s context is “refreshed” by dropping older content, this number of tokens from the start (usually the system prompt or important instructions) will be retained. By default only a few tokens (e.g. 4) are kept – typically just enough to preserve the initial instruction or format. Increase this if you have a long system prompt or crucial information at the start that must persist across a very lengthy chat. Be mindful that keeping more tokens means slightly less room for new conversation, so only raise it if necessary. |
| Max Tokens (num_predict)                                      | Max Tokens (Output Length)                  | The maximum number of tokens the model will generate for the response. This essentially caps how long the answer can be. A higher limit allows longer, more detailed responses, while a lower limit can keep answers brief and to-the-point. By default, this may be set high (or even unlimited) so the model can elaborate as needed. You may want to lower it for tasks where you explicitly need short answers or to save computation time. For instance, setting a max token limit is useful if you only want a summary of a fixed length. If the model reaches this limit, it will stop even if it hasn’t naturally finished the answer.                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Repeat Penalty (Ollama)                                       | Repetition Penalty                           | A general penalty factor for repeated tokens in local (Ollama) models. Values >1.0 discourage repetition (making each repeated word exponentially less likely). For example, at 1.5 the model will strongly avoid reusing exact words or phrases too often. A value of 1.0 means no penalty (the model treats repeats normally). This is similar in purpose to frequency/presence penalties, but implemented as a single scaling factor. The default is usually slightly above 1 (e.g. 1.1) to gently reduce repetition. Increase it (1.2–1.5) if the model starts looping or copying itself; decrease back toward 1.0 if you find the output is unnaturally avoiding necessary repetition.                                                                                                                                                                                                                                                                                                                                                                                     |
| Context Length (Ollama)                                       | Context Length                               | The maximum number of tokens the model can hold in its “memory” (context window) for a single prompt or conversation. This includes the input prompt plus recent conversation history. A larger context length allows the model to take more prior text into account (good for long dialogues or documents), but also uses more memory. Most models have a fixed context limit (e.g. 2048 tokens by default). In Open WebUI, you typically don’t need to change this – it’s set to the model’s capacity. Only reduce it if you have a specific reason (like performance constraints), and be aware that if the conversation exceeds this length, older messages will be forgotten.                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Batch Size (num_batch)                                        | Batch Size                                   | Number of tokens processed in one go during generation (for local models). A higher batch size means the model looks at more tokens at once, which can speed up generation if your hardware can handle it, but it uses more RAM/VRAM. The default is 512 for many models, balancing speed and memory. If you run into memory issues or crashes with long prompts or big models, you can lower this (e.g. 256) to reduce memory usage. Lower batch size might slow down the response slightly. In short, increase it for performance if you have ample memory; decrease it if you need to conserve memory or avoid out‑of‑memory errors.                                                                                                                                                                                                                                                                                                                                                                                                                          |
| use_mmap (Ollama)                                              | Memory Map Model (use_mmap)                  | Controls memory‑mapping of the model file. When enabled (default on most systems), the model is loaded via the operating system’s memory‑map, which means parts of the model are loaded into RAM on‑demand rather than all at once. This can make initial load faster and use less RAM upfront, but if your system is low on memory or the model is larger than RAM, it may cause a lot of slow disk swapping (pageouts). Disabling this will load the entire model into RAM immediately (using more memory but potentially avoiding swapping later). In general, leave this on for normal use. You might turn it off if you have plenty of RAM and want to ensure the whole model is resident (which can avoid stutters), or if memory‑mapping is causing performance issues on your system.                                                                                                                                                                                                                                                                 |
| use_mlock (Ollama)                                             | Lock Memory (use_mlock)                      | If enabled, forces the model’s data to stay in RAM without being swapped out to disk. This “locks” the model in memory. It can improve steady‑state performance (no pauses due to swapping) but requires enough RAM to hold the entire model and may slow down the initial load as the data is pinned in memory. By default this is off (False). Consider turning it on only if you notice the model getting swapped to disk (which can make generation choppy or slow) and you have sufficient free memory to keep it all in RAM. If RAM is limited, keep this off to allow the system to swap if needed (otherwise you could run out of memory).                                                                                                                                                                                                                                                                                                                                   |
